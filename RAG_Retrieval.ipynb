{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6e1096-1f36-4711-a5c7-7612b6e771cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da79f95-3a6c-4fd5-8a58-012b891ede9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b2f1ef-234e-4677-aec9-7b3c5ebf8e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c25884-248a-425d-b552-fccd4b9feb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "azure_model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30cea834-03a7-4955-9710-7ec80a8e8e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 10:28:33.479212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f9a50a-967d-432e-9ade-5221728167af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "vector_size = len(embeddings.embed_query(\"sample text\"))\n",
    "\n",
    "if not client.collection_exists(\"test\"):\n",
    "    client.create_collection(\n",
    "        collection_name=\"test\",\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "    )\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"test\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc6a1fe-d787-4307-836c-f839627d5f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0xbbee for key /x1098\n",
      "Multiple definitions in dictionary at byte 0xbbfe for key /x1099\n",
      "Multiple definitions in dictionary at byte 0xbc0e for key /x1098\n",
      "Multiple definitions in dictionary at byte 0xbc1e for key /x1099\n",
      "Multiple definitions in dictionary at byte 0xbc2e for key /x1098\n",
      "Multiple definitions in dictionary at byte 0xbc3e for key /x1099\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "all_docs = []\n",
    "for file in os.listdir(\"PDFs\"):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(\"PDFs\", file))\n",
    "        docs = loader.load()\n",
    "        all_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe883cf1-35fc-43f9-ab2c-a27b94b16ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 2312 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(all_docs)\n",
    "print(f\"Split into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79dfc9b-d847-42ff-a842-9f56a08ebea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ffc75a0f3b69473a8f6673d9f7f524eb', 'cac66999420d48c4877ee61a01e67b50', '3d9780b0ef28417ea9dacc5b8cecf0e5']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ae66391-91b3-4858-bfd2-26b6a01d634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sheet (user_input, to_print=True):\n",
    "    \"\"\"\n",
    "    Must be called alongside user input every time. Returns scores per pdf.\n",
    "    \"\"\"\n",
    "    results = vector_store.similarity_search_with_score(user_input, k=1000)\n",
    "    \n",
    "    pdf_best_scores = {}\n",
    "    for doc, score in results:\n",
    "        src = doc.metadata[\"source\"]\n",
    "        pdf_best_scores[src] = max(pdf_best_scores.get(src, 0), score)\n",
    "    \n",
    "    files = np.array(list(pdf_best_scores.keys()))\n",
    "    scores = np.array(list(pdf_best_scores.values()))\n",
    "    df = pd.DataFrame(np.concatenate((files.reshape(-1,1),scores.reshape(-1,1)), axis=1), columns=['1','2'])\n",
    "\n",
    "    if to_print:\n",
    "        print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02556841-4c04-48b3-ae87-33f80e4bebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56c25da2-13c7-46a1-b91e-befe03e7e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranking(user_input, k=2):\n",
    "    df = score_sheet(user_input)\n",
    "    df_rerank = df.iloc[:k,0].values\n",
    "\n",
    "    reranked_docs = []\n",
    "    for doc in df_rerank:\n",
    "        loader = PyPDFLoader(doc)\n",
    "        docs = loader.load()\n",
    "        reranked_docs.extend(docs)\n",
    "        \n",
    "    text_splitter2 = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "    reranked_splits = text_splitter.split_documents(reranked_docs)\n",
    "    \n",
    "    query_vec = embeddings.embed_query(user_input)\n",
    "    similarity_value = []\n",
    "    for i in range(len(reranked_splits)):\n",
    "        print(i)\n",
    "        vec2 = embeddings.embed_query(reranked_splits[i].page_content)\n",
    "        similarity_value.append(cosine_similarity(query_vec, vec2))\n",
    "    similarity_value = np.array(similarity_value)\n",
    "    return np.max(similarity_value), reranked_splits[np.argmax(similarity_value)].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "110d56cb-f46d-464b-bad1-78d8f5e1e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "retrieval_cache = {}\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    key = query.strip().lower()\n",
    "    if key in retrieval_cache:\n",
    "        print(\"Calling from cache...\")\n",
    "        return retrieval_cache[key]\n",
    "\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    \n",
    "    serialized = \"\\n\\n\".join(\n",
    "        f\"Source: {doc.metadata}\\nContent: {doc.page_content}\"\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "\n",
    "    score_sheet(query)\n",
    "    \n",
    "    result = (serialized, retrieved_docs)\n",
    "    retrieval_cache[key] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58304ee3-07de-4287-8210-514c7bebd931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    1                    2\n",
      "0                            PDFs/Dog - Wikipedia.pdf    0.590761826641792\n",
      "1                         PDFs/Animal - Wikipedia.pdf  0.38099446675119253\n",
      "2        PDFs/Artificial intelligence - Wikipedia.pdf   0.2858703229333798\n",
      "3                        PDFs/Chatbot - Wikipedia.pdf  0.25428517534330786\n",
      "4               PDFs/Machine learning - Wikipedia.pdf  0.23769084409651214\n",
      "5                      PDFs/LangChain - Wikipedia.pdf   0.2274744174967486\n",
      "6   PDFs/Generative artificial intelligence - Wiki...   0.2210317352859998\n",
      "7                  PDFs/Search engine - Wikipedia.pdf  0.21752973444491774\n",
      "8           PDFs/Large language model - Wikipedia.pdf   0.2165383670288706\n",
      "9            PDFs/Supervised learning - Wikipedia.pdf   0.1878379323025521\n",
      "10                 PDFs/Heat transfer - Wikipedia.pdf  0.15452958853763915\n",
      "11              PDFs/Search algorithm - Wikipedia.pdf  0.15090143233093228\n",
      "12  PDFs/Retrieval-augmented generation - Wikipedi...  0.14859635654614523\n",
      "13               PDFs/Albert Einstein - Wikipedia.pdf   0.1431595747572882\n",
      "14                  PDFs/Albert Camus - Wikipedia.pdf  0.10732558937863379\n"
     ]
    }
   ],
   "source": [
    "serialized, retrieved_docs = retrieve_context(\"What is a dog?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fccc8356-614e-48ea-9c50-e5f8eda4834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.caches import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "tools = [retrieve_context]\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from PDF Documents.\"\n",
    "    \"Use the tool to help answer user queries. Mention to the user when you are using cached data.\"\n",
    ")\n",
    "\n",
    "agent = create_agent(azure_model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fce45c81-0add-498a-ad4a-6b1d70ea0212",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "query = (\n",
    "    \"What is a dog?\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    checkpointer=InMemorySaver(),\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc4dc749-fe70-4f4f-9fb1-83ad33ceff19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Prompt (type 'q' to quit, type 'n' for new conversation):  what is net zero?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is net zero?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\"Net zero\" refers to balancing the amount of greenhouse gases emitted into the atmosphere with an equivalent amount of emissions removed or offset, resulting in a net zero increase in atmospheric greenhouse gases. The goal is to reduce carbon emissions to as close to zero as possible and offset any remaining emissions through measures like carbon capture or reforestation. This concept is central to efforts to combat climate change by limiting global temperature rise. \n",
      "\n",
      "Would you like more detailed information or specific examples?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Prompt (type 'q' to quit, type 'n' for new conversation):  what is langchain?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is langchain?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_4H8bQQ5bCM4klN9s3PJweJss)\n",
      " Call ID: call_4H8bQQ5bCM4klN9s3PJweJss\n",
      "  Args:\n",
      "    query: LangChain\n",
      "                                                    1                    2\n",
      "0                      PDFs/LangChain - Wikipedia.pdf   0.7271866299076896\n",
      "1           PDFs/Large language model - Wikipedia.pdf   0.4358545416032088\n",
      "2   PDFs/Generative artificial intelligence - Wiki...  0.36982122712988863\n",
      "3               PDFs/Machine learning - Wikipedia.pdf   0.3412312466703556\n",
      "4                        PDFs/Chatbot - Wikipedia.pdf  0.30900071473973856\n",
      "5        PDFs/Artificial intelligence - Wikipedia.pdf  0.30751075532570876\n",
      "6   PDFs/Retrieval-augmented generation - Wikipedi...   0.2959433762944227\n",
      "7                  PDFs/Search engine - Wikipedia.pdf  0.28413981862948656\n",
      "8                         PDFs/Animal - Wikipedia.pdf  0.24081434495642673\n",
      "9                            PDFs/Dog - Wikipedia.pdf  0.23216899066468424\n",
      "10               PDFs/Albert Einstein - Wikipedia.pdf   0.1771133118559588\n",
      "11           PDFs/Supervised learning - Wikipedia.pdf  0.17118370372377006\n",
      "12                  PDFs/Albert Camus - Wikipedia.pdf  0.15618818150771116\n",
      "13              PDFs/Search algorithm - Wikipedia.pdf  0.14703096710192884\n",
      "14  PDFs/Intergovernmental Panel on Climate Change...  0.12486770069723678\n",
      "15            PDFs/Net-zero emissions - Wikipedia.pdf  0.11457838937372535\n",
      "16                PDFs/Nusselt number - Wikipedia.pdf  0.11323181160296664\n",
      "17                 PDFs/Heat transfer - Wikipedia.pdf  0.10419844629504588\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'producer': 'cairo 1.18.0 (https://cairographics.org)', 'creator': 'Mozilla Firefox 146.0.1', 'creationdate': '2026-01-09T10:52:21+05:30', 'source': 'PDFs/LangChain - Wikipedia.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'start_index': 0, '_id': '5fb91b7b39574b39b83de9ff99f179cd', '_collection_name': 'test'}\n",
      "Content: LangChain\n",
      "DeveloperHarrison Chase\n",
      "Initial releaseOctober 2022\n",
      "Stable release0.1.16 [1] / 11 April 2024\n",
      "Repository github.com/langchain-ai/\n",
      "langchain (https://githu\n",
      "b.com/langchain-ai/lang\n",
      "chain)\n",
      "Written in Python and JavaScript\n",
      "Type Software framework for\n",
      "large language model\n",
      "application development\n",
      "License MIT License\n",
      "Website LangChain.com (https://l\n",
      "angchain.com/)\n",
      "LangChain\n",
      "Free and open-\n",
      "source software\n",
      "portal\n",
      "LangChain is  a  software  framework  that  helps\n",
      "facilitate  the  integration  of  large  language  models\n",
      "(LLMs)  into  applications.  As  a  language  model\n",
      "integration framework, LangChain's use-cases largely\n",
      "overlap  with  those  of  language  models  in  general,\n",
      "including  document  analysis  and  summarization,\n",
      "chatbots, and code analysis.[2]\n",
      "LangChain was launched in October 2022 as an open\n",
      "source  project  by  Harrison  Chase,  while  working  at\n",
      "machine learning startup Robust Intelligence. In April\n",
      "2023, LangChain had incorporated and the new startup\n",
      "\n",
      "Source: {'producer': 'cairo 1.18.0 (https://cairographics.org)', 'creator': 'Mozilla Firefox 146.0.1', 'creationdate': '2026-01-09T10:52:21+05:30', 'source': 'PDFs/LangChain - Wikipedia.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'start_index': 1641, '_id': '60b80a9120cd4fc39fd2f4f0e05074f4', '_collection_name': 'test'}\n",
      "Content: 2025  the  company  launched  LangGraph  Platform  into  general  availability,  providing  managed\n",
      "infrastructure for deploying long-running, stateful AI agents.[9]\n",
      "LangChain's  developers  highlight  the  framework's  applicability  to  use-cases  including  chatbots,[10]\n",
      "retrieval-augmented generation,[11] document summarization,[12] and synthetic data generation.[13]\n",
      "As  of  March  2023,  LangChain  included  integrations  with  systems  including  Amazon,  Google,  and\n",
      "Microsoft Azure cloud storage;[14] API wrappers for news, movie information, and weather; Bash  for\n",
      "summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping\n",
      "History\n",
      "Capabilities\n",
      "LangChain - Wikipedia https://en.wikipedia.org/wiki/LangChain\n",
      "1 of 4 09/01/26, 10:52\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangChain is a software framework designed to facilitate the integration of large language models (LLMs) into applications. It is written in Python and JavaScript and is open-source under the MIT License. Developed by Harrison Chase and launched in October 2022, LangChain enables developers to build applications that leverage the capabilities of LLMs for various tasks such as document analysis, summarization, chatbots, code analysis, retrieval-augmented generation, and synthetic data generation. The framework supports integration with cloud services like Amazon, Google, and Microsoft Azure, and includes features for web scraping and data retrieval.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Prompt (type 'q' to quit, type 'n' for new conversation):  what is langchain?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is langchain?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_ULzbw1Kehyi1q9y3tE6aYYJB)\n",
      " Call ID: call_ULzbw1Kehyi1q9y3tE6aYYJB\n",
      "  Args:\n",
      "    query: langchain\n",
      "Calling from cache...\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'producer': 'cairo 1.18.0 (https://cairographics.org)', 'creator': 'Mozilla Firefox 146.0.1', 'creationdate': '2026-01-09T10:52:21+05:30', 'source': 'PDFs/LangChain - Wikipedia.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'start_index': 0, '_id': '5fb91b7b39574b39b83de9ff99f179cd', '_collection_name': 'test'}\n",
      "Content: LangChain\n",
      "DeveloperHarrison Chase\n",
      "Initial releaseOctober 2022\n",
      "Stable release0.1.16 [1] / 11 April 2024\n",
      "Repository github.com/langchain-ai/\n",
      "langchain (https://githu\n",
      "b.com/langchain-ai/lang\n",
      "chain)\n",
      "Written in Python and JavaScript\n",
      "Type Software framework for\n",
      "large language model\n",
      "application development\n",
      "License MIT License\n",
      "Website LangChain.com (https://l\n",
      "angchain.com/)\n",
      "LangChain\n",
      "Free and open-\n",
      "source software\n",
      "portal\n",
      "LangChain is  a  software  framework  that  helps\n",
      "facilitate  the  integration  of  large  language  models\n",
      "(LLMs)  into  applications.  As  a  language  model\n",
      "integration framework, LangChain's use-cases largely\n",
      "overlap  with  those  of  language  models  in  general,\n",
      "including  document  analysis  and  summarization,\n",
      "chatbots, and code analysis.[2]\n",
      "LangChain was launched in October 2022 as an open\n",
      "source  project  by  Harrison  Chase,  while  working  at\n",
      "machine learning startup Robust Intelligence. In April\n",
      "2023, LangChain had incorporated and the new startup\n",
      "\n",
      "Source: {'producer': 'cairo 1.18.0 (https://cairographics.org)', 'creator': 'Mozilla Firefox 146.0.1', 'creationdate': '2026-01-09T10:52:21+05:30', 'source': 'PDFs/LangChain - Wikipedia.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'start_index': 1641, '_id': '60b80a9120cd4fc39fd2f4f0e05074f4', '_collection_name': 'test'}\n",
      "Content: 2025  the  company  launched  LangGraph  Platform  into  general  availability,  providing  managed\n",
      "infrastructure for deploying long-running, stateful AI agents.[9]\n",
      "LangChain's  developers  highlight  the  framework's  applicability  to  use-cases  including  chatbots,[10]\n",
      "retrieval-augmented generation,[11] document summarization,[12] and synthetic data generation.[13]\n",
      "As  of  March  2023,  LangChain  included  integrations  with  systems  including  Amazon,  Google,  and\n",
      "Microsoft Azure cloud storage;[14] API wrappers for news, movie information, and weather; Bash  for\n",
      "summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping\n",
      "History\n",
      "Capabilities\n",
      "LangChain - Wikipedia https://en.wikipedia.org/wiki/LangChain\n",
      "1 of 4 09/01/26, 10:52\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangChain is a software framework designed to facilitate the integration of large language models (LLMs) into applications. It was developed by Harrison Chase and was first released in October 2022. LangChain supports various use cases including document analysis and summarization, chatbots, code analysis, retrieval-augmented generation, synthetic data generation, and more. The framework is written in Python and JavaScript and is open-source under the MIT License. It provides tools and integrations with cloud storage systems like Amazon, Google, and Microsoft Azure, as well as APIs for news, weather, and web scraping. You can find more information about it on their website [LangChain.com](https://langchain.com/).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Prompt (type 'q' to quit, type 'n' for new conversation):  q\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting PDF analyst.\n"
     ]
    }
   ],
   "source": [
    "i='1'\n",
    "while True:\n",
    "    user_input = input(\"Enter Prompt (type 'q' to quit, type 'n' for new conversation): \").strip()\n",
    "    \n",
    "    if user_input.lower() in [\"q\", \"quit\"]:\n",
    "        print(\"Exiting PDF analyst.\")\n",
    "        break\n",
    "\n",
    "    if user_input.lower() in [\"n\", \"new\"]:\n",
    "        print(\"What can I do for you?\")\n",
    "        i = str(int(i) + 1)\n",
    "        continue\n",
    "\n",
    "    steps = []\n",
    "    \n",
    "    for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "    checkpointer=InMemorySaver(),\n",
    "    stream_mode=\"values\",\n",
    "    ):\n",
    "        event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9365859-7bf5-4ee5-941b-338055f5823a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
